{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Step 1: Preprocess the Text\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "    return sentences, tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 2: Load or Train Word2Vec Model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# You can use a pre-trained Word2Vec model or train your own on a relevant corpus\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(\u001b[43mtokenized_sentences\u001b[49m, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Step 3: Compute Sentence Embeddings\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_embedding\u001b[39m(sentence, model):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Load or Train Word2Vec Model\n",
    "# You can use a pre-trained Word2Vec model or train your own on a relevant corpus\n",
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 3: Compute Sentence Embeddings\n",
    "def sentence_embedding(sentence, model):\n",
    "    words = [word for word in sentence if word in model.wv]\n",
    "    if words:\n",
    "        return np.mean(model.wv[words], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentence_embeddings \u001b[38;5;241m=\u001b[39m [sentence_embedding(sentence, model) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenized_sentences\u001b[49m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Step 4: Compute Document Embedding\u001b[39;00m\n\u001b[0;32m      4\u001b[0m document_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(sentence_embeddings, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_embeddings = [sentence_embedding(sentence, model) for sentence in tokenized_sentences]\n",
    "\n",
    "# Step 4: Compute Document Embedding\n",
    "document_embedding = np.mean(sentence_embeddings, axis=0)\n",
    "\n",
    "# Step 5: Rank Sentences by Importance\n",
    "similarity_scores = [cosine_similarity([embedding], [document_embedding])[0][0] for embedding in sentence_embeddings]\n",
    "ranked_sentences = sorted(((score, idx) for idx, score in enumerate(similarity_scores)), reverse=True)\n",
    "\n",
    "# Step 6: Select Top Sentences\n",
    "# You can decide how many sentences you want in the summary\n",
    "top_n = 5\n",
    "top_sentence_indices = [idx for score, idx in ranked_sentences[:top_n]]\n",
    "top_sentence_indices.sort()\n",
    "\n",
    "# Step 7: Extract and Return the Summarized Text\n",
    "def extract_summary(sentences, indices):\n",
    "    return ' '.join([sentences[idx] for idx in indices])\n",
    "\n",
    "text = \"Your input text here.\"\n",
    "sentences, tokenized_sentences = preprocess_text(text)\n",
    "summary = extract_summary(sentences, top_sentence_indices)\n",
    "\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
