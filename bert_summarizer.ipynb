{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYour input text goes here. It should be a long paragraph with multiple sentences.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     53\u001b[0m keywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 55\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\u001b[39m, summary)\n",
      "Cell \u001b[1;32mIn[17], line 47\u001b[0m, in \u001b[0;36msummarize\u001b[1;34m(text, keywords, num_sentences)\u001b[0m\n\u001b[0;32m     44\u001b[0m relevant_sentences \u001b[38;5;241m=\u001b[39m keyword_relevant_sentences(sentences, keywords)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Ensure unique sentences in the final summary\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m final_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrelevant_sentences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(final_summary)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained model tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    sentences = text.split('. ')\n",
    "    return sentences\n",
    "\n",
    "def get_sentence_embeddings(sentences, tokenizer, model):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return embeddings\n",
    "\n",
    "def rank_sentences(sentences, embeddings):\n",
    "    sentence_scores = []\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        score = cosine_similarity([emb], embeddings).mean()\n",
    "        sentence_scores.append((score, i))\n",
    "    ranked_sentences = sorted(sentence_scores, reverse=True, key=lambda x: x[0])\n",
    "    return [sentences[i] for _, i in ranked_sentences]\n",
    "\n",
    "def keyword_relevant_sentences(sentences, keywords):\n",
    "    relevant_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if any(keyword.lower() in sentence.lower() for keyword in keywords):\n",
    "            relevant_sentences.append(sentence)\n",
    "    return relevant_sentences\n",
    "\n",
    "def summarize(text, keywords, num_sentences=5):\n",
    "    sentences = preprocess_text(text)\n",
    "    embeddings = get_sentence_embeddings(sentences, tokenizer, model)\n",
    "    ranked_sentences = rank_sentences(sentences, embeddings)\n",
    "    \n",
    "    summary = ranked_sentences[:num_sentences]\n",
    "    relevant_sentences = keyword_relevant_sentences(sentences, keywords)\n",
    "    \n",
    "    # Ensure unique sentences in the final summary\n",
    "    final_summary = list(set(summary + relevant_sentences))\n",
    "    \n",
    "    return '. '.join(final_summary)\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"Your input text goes here. It should be a long paragraph with multiple sentences.\"\"\"\n",
    "keywords = [\"keyword1\", \"keyword2\"]\n",
    "\n",
    "summary = summarize(text, keywords)\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = summary.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = text.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your input text goes here',\n",
       " ' It should be a long paragraph with multiple sentences',\n",
       " '']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Climate change is one of the most pressing issues of our time',\n",
       " ' Rising global temperatures have led to a variety of environmental impacts, including more frequent and severe weather events',\n",
       " ' The polar ice caps are melting at an alarming rate, causing sea levels to rise and threatening coastal communities',\n",
       " ' Governments and organizations around the world are working to address climate change through various measures',\n",
       " ' Renewable energy sources, such as solar and wind power, are being developed to reduce reliance on fossil fuels',\n",
       " ' International agreements, like the Paris Agreement, aim to unite countries in the fight against climate change',\n",
       " ' It is essential that everyone takes part in efforts to mitigate the effects of climate change to ensure a sustainable future for generations to come',\n",
       " '\\n ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A paragraph is a series of sentences that are organized and coherent .\n",
      "that are organized coherent\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def compound_to_simple(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    simple_sentences = []\n",
    "\n",
    "    # Identify root and dependent clauses\n",
    "    for sent in doc.sents:\n",
    "        root = [token for token in sent if token.dep_ == \"ROOT\"][0]\n",
    "        clause_1 = \" \".join([token.text for token in root.subtree])\n",
    "        simple_sentences.append(clause_1)\n",
    "\n",
    "        # Handle conjunctions\n",
    "        for token in sent:\n",
    "            if token.dep_ == \"cc\":  # coordinating conjunction\n",
    "                conj_head = token.head\n",
    "                clause_2 = \" \".join([child.text for child in conj_head.subtree if child != token])\n",
    "                simple_sentences.append(clause_2)\n",
    "\n",
    "    return simple_sentences\n",
    "\n",
    "# Example usage\n",
    "compound_sentence = \"A paragraph is a series of sentences that are organized and coherent.\"\n",
    "simple_sentences = compound_to_simple(compound_sentence)\n",
    "for simple_sentence in simple_sentences:\n",
    "    print(simple_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input - \"A paragraph is a series of sentences that are organized and coherent.\"\n",
      "output - \"A paragraph is a series of sentences that are organized\"\n",
      "output - \"A paragraph is a series of sentences that are organized\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def compound_to_simple(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    simple_sentences = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        # Get the part of the sentence before the conjunction or relative clause\n",
    "        before_clause = []\n",
    "        clause_parts = []\n",
    "        in_clause = False\n",
    "\n",
    "        for token in sent:\n",
    "            if token.dep_ in {\"nsubj\", \"nsubjpass\"} and not in_clause:\n",
    "                before_clause.append(token.text)\n",
    "            elif token.dep_ == \"cc\" or token.dep_ == \"conj\":\n",
    "                in_clause = True\n",
    "                clause_parts.append(token.text)\n",
    "            elif in_clause:\n",
    "                clause_parts.append(token.text)\n",
    "            else:\n",
    "                before_clause.append(token.text)\n",
    "        \n",
    "        # Remove the conjunction (e.g., 'and') from the clause parts\n",
    "        if clause_parts and clause_parts[0] in {\"and\", \"or\", \"but\"}:\n",
    "            clause_parts.pop(0)\n",
    "\n",
    "        before_clause_text = \" \".join(before_clause)\n",
    "        clause_text = \" \".join(clause_parts)\n",
    "\n",
    "        if clause_text:\n",
    "            clause_1 = f\"{before_clause_text}\"\n",
    "            clause_2 = f\"{before_clause_text}\"\n",
    "            simple_sentences.append(clause_1)\n",
    "            simple_sentences.append(clause_2)\n",
    "\n",
    "    return simple_sentences\n",
    "\n",
    "# Example usage\n",
    "compound_sentence = \"A paragraph is a series of sentences that are organized and coherent.\"\n",
    "simple_sentences = compound_to_simple(compound_sentence)\n",
    "\n",
    "print(f\"input - \\\"{compound_sentence}\\\"\")\n",
    "for simple_sentence in simple_sentences:\n",
    "    print(f\"output - \\\"{simple_sentence}\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Sentence 1: It is raining like cats and\n",
      "Simple Sentence 2: dogs\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def compound_to_simple(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extract the conjunctions and split the sentence based on them\n",
    "    simple_sentences = []\n",
    "    temp_sentence = []\n",
    "\n",
    "    for token in doc:\n",
    "        temp_sentence.append(token.text)\n",
    "        if token.dep_ == 'cc' or token.dep_ == 'punct':\n",
    "            simple_sentences.append(' '.join(temp_sentence).strip())\n",
    "            temp_sentence = []\n",
    "    \n",
    "    if temp_sentence:\n",
    "        simple_sentences.append(' '.join(temp_sentence).strip())\n",
    "    \n",
    "    # Further split based on commas and semicolons if needed\n",
    "    final_sentences = []\n",
    "    for sent in simple_sentences:\n",
    "        sub_sentences = nltk.sent_tokenize(sent.replace(',', '.').replace(';', '.'))\n",
    "        final_sentences.extend(sub_sentences)\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "# Example usage\n",
    "compound_sentence = \"It is raining like cats and dogs\"\n",
    "simple_sentences = compound_to_simple(compound_sentence)\n",
    "\n",
    "for idx, sent in enumerate(simple_sentences):\n",
    "    print(f\"Simple Sentence {idx+1}: {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Sentence 1: It is raining like dogs .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_by_conjunctions(sentence):\n",
    "    \"\"\"\n",
    "    Split a sentence by its conjunctions and return the part before the conjunction\n",
    "    and the phrases/words after the conjunction.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    before_conjunction = []\n",
    "    after_conjunctions = []\n",
    "    temp_sentence = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'cc':  # Coordinating conjunction\n",
    "            before_conjunction.append(' '.join(temp_sentence).strip())\n",
    "            temp_sentence = []\n",
    "        else:\n",
    "            temp_sentence.append(token.text)\n",
    "    \n",
    "    if temp_sentence:\n",
    "        after_conjunctions.append(' '.join(temp_sentence).strip())\n",
    "    \n",
    "    return before_conjunction, after_conjunctions\n",
    "\n",
    "def expand_phrases(before_conjunction, after_conjunctions):\n",
    "    \"\"\"\n",
    "    Expand phrases to form new simple sentences based on the parts before and after conjunctions.\n",
    "    \"\"\"\n",
    "    expanded_sentences = []\n",
    "    for part in before_conjunction:\n",
    "        if 'like' in part:\n",
    "            parts = part.split('like')\n",
    "            if len(parts) > 1:\n",
    "                for after_part in after_conjunctions:\n",
    "                    subjects = after_part.split('and')\n",
    "                    for subject in subjects:\n",
    "                        expanded_sentences.append(parts[0].strip() + ' like ' + subject.strip())\n",
    "            else:\n",
    "                expanded_sentences.append(part)\n",
    "        else:\n",
    "            for after_part in after_conjunctions:\n",
    "                expanded_sentences.append(part + ' ' + after_part)\n",
    "    return expanded_sentences\n",
    "\n",
    "def compound_to_simple(sentence):\n",
    "    # Split the sentence by conjunctions\n",
    "    before_conjunction, after_conjunctions = split_by_conjunctions(sentence)\n",
    "    \n",
    "    # Expand phrases based on the parts before and after conjunctions\n",
    "    final_sentences = expand_phrases(before_conjunction, after_conjunctions)\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "# Example usage\n",
    "compound_sentence = \"It is raining like cats and dogs.\"\n",
    "simple_sentences = compound_to_simple(compound_sentence)\n",
    "\n",
    "for idx, sent in enumerate(simple_sentences):\n",
    "    print(f\"Simple Sentence {idx+1}: {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before conjunction(s): ['A paragraph is a series of sentences that are organized']\n",
      "After conjunction(s): ['coherent .']\n",
      "Simple Sentence 1: A paragraph is a series of sentences that are organized coherent .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_by_conjunctions(sentence):\n",
    "    \"\"\"\n",
    "    Split a sentence by its conjunctions and return the part before the conjunction\n",
    "    and the phrases/words after the conjunction.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    before_conjunction = []\n",
    "    after_conjunctions = []\n",
    "    temp_sentence = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'cc':  # Coordinating conjunction\n",
    "            before_conjunction.append(' '.join(temp_sentence).strip())\n",
    "            temp_sentence = []\n",
    "        else:\n",
    "            temp_sentence.append(token.text)\n",
    "    \n",
    "    if temp_sentence:\n",
    "        after_conjunctions.append(' '.join(temp_sentence).strip())\n",
    "    \n",
    "    return before_conjunction, after_conjunctions\n",
    "\n",
    "def expand_phrases(before_conjunction, after_conjunctions):\n",
    "    \"\"\"\n",
    "    Expand phrases to form new simple sentences based on the parts before and after conjunctions.\n",
    "    \"\"\"\n",
    "    expanded_sentences = []\n",
    "    for part in before_conjunction:\n",
    "        if 'like' in part:\n",
    "            parts = part.split('like')\n",
    "            if len(parts) > 1:\n",
    "                for after_part in after_conjunctions:\n",
    "                    subjects = after_part.split('and')\n",
    "                    for subject in subjects:\n",
    "                        expanded_sentences.append(parts[0].strip() + ' like ' + subject.strip())\n",
    "            else:\n",
    "                expanded_sentences.append(part)\n",
    "        else:\n",
    "            for after_part in after_conjunctions:\n",
    "                expanded_sentences.append(part + ' ' + after_part)\n",
    "    return expanded_sentences\n",
    "\n",
    "def compound_to_simple(sentence):\n",
    "    # Split the sentence by conjunctions\n",
    "    before_conjunction, after_conjunctions = split_by_conjunctions(sentence)\n",
    "    \n",
    "    # Print parts before and after conjunctions\n",
    "    print(\"Before conjunction(s):\", before_conjunction)\n",
    "    print(\"After conjunction(s):\", after_conjunctions)\n",
    "    \n",
    "    # Expand phrases based on the parts before and after conjunctions\n",
    "    final_sentences = expand_phrases(before_conjunction, after_conjunctions)\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "# Example usage\n",
    "compound_sentence = \"A paragraph is a series of sentences that are organized and coherent.\"\n",
    "simple_sentences = compound_to_simple(compound_sentence)\n",
    "\n",
    "for idx, sent in enumerate(simple_sentences):\n",
    "    print(f\"Simple Sentence {idx+1}: {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "cats.\n",
      "cats dogs\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def split_compound_sentence(sentence):\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Identify the conjunction and split the sentence\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"CCONJ\":\n",
    "            # Find the start and end of the first part\n",
    "            start = token.head.left_edge.i\n",
    "            end = token.head.i\n",
    "            \n",
    "            first_part = doc[start:end+1].text\n",
    "            \n",
    "            # Create two new sentences\n",
    "            first_sentence = first_part + \".\"\n",
    "            second_sentence = first_part.rsplit(maxsplit=1)[0] + \" \" + doc[end+2:].text\n",
    "            \n",
    "            return first_sentence, second_sentence\n",
    "\n",
    "# Example usage\n",
    "input_sentence = \"It is raining like cats and dogs\"\n",
    "output_sentences = split_compound_sentence(input_sentence)\n",
    "print(\"Output:\")\n",
    "print(output_sentences[0])\n",
    "print(output_sentences[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
