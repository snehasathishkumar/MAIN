{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A paragraph is a series of sentences that are organized and coherent .\n",
      "that are organized coherent\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def compound_to_simple(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    simple_sentences = []\n",
    "\n",
    "    # Identify root and dependent clauses\n",
    "    for sent in doc.sents:\n",
    "        root = [token for token in sent if token.dep_ == \"ROOT\"][0]\n",
    "        clause_1 = \" \".join([token.text for token in root.subtree])\n",
    "        simple_sentences.append(clause_1)\n",
    "\n",
    "        # Handle conjunctions\n",
    "        for token in sent:\n",
    "            if token.dep_ == \"cc\":  # coordinating conjunction\n",
    "                conj_head = token.head\n",
    "                clause_2 = \" \".join([child.text for child in conj_head.subtree if child != token])\n",
    "                simple_sentences.append(clause_2)\n",
    "\n",
    "    return simple_sentences\n",
    "\n",
    "# Example usage\n",
    "compound_sentence = \"A paragraph is a series of sentences that are organized and coherent.\"\n",
    "simple_sentences = compound_to_simple(compound_sentence)\n",
    "for simple_sentence in simple_sentences:\n",
    "    print(simple_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input - \"A paragraph is a series of sentences that are organized and coherent.\"\n",
      "output - \"A paragraph is a series of sentences that are organized\"\n",
      "output - \"A paragraph is a series of sentences that are organized\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def compound_to_simple(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    simple_sentences = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        # Get the part of the sentence before the conjunction or relative clause\n",
    "        before_clause = []\n",
    "        clause_parts = []\n",
    "        in_clause = False\n",
    "\n",
    "        for token in sent:\n",
    "            if token.dep_ in {\"nsubj\", \"nsubjpass\"} and not in_clause:\n",
    "                before_clause.append(token.text)\n",
    "            elif token.dep_ == \"cc\" or token.dep_ == \"conj\":\n",
    "                in_clause = True\n",
    "                clause_parts.append(token.text)\n",
    "            elif in_clause:\n",
    "                clause_parts.append(token.text)\n",
    "            else:\n",
    "                before_clause.append(token.text)\n",
    "        \n",
    "        # Remove the conjunction (e.g., 'and') from the clause parts\n",
    "        if clause_parts and clause_parts[0] in {\"and\", \"or\", \"but\"}:\n",
    "            clause_parts.pop(0)\n",
    "\n",
    "        before_clause_text = \" \".join(before_clause)\n",
    "        clause_text = \" \".join(clause_parts)\n",
    "\n",
    "        if clause_text:\n",
    "            clause_1 = f\"{before_clause_text}\"\n",
    "            clause_2 = f\"{before_clause_text}\"\n",
    "            simple_sentences.append(clause_1)\n",
    "            simple_sentences.append(clause_2)\n",
    "\n",
    "    return simple_sentences\n",
    "\n",
    "# Example usage\n",
    "compound_sentence = \"A paragraph is a series of sentences that are organized and coherent.\"\n",
    "simple_sentences = compound_to_simple(compound_sentence)\n",
    "\n",
    "print(f\"input - \\\"{compound_sentence}\\\"\")\n",
    "for simple_sentence in simple_sentences:\n",
    "    print(f\"output - \\\"{simple_sentence}\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Sentence 1: It is raining like cats and\n",
      "Simple Sentence 2: dogs\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def compound_to_simple(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extract the conjunctions and split the sentence based on them\n",
    "    simple_sentences = []\n",
    "    temp_sentence = []\n",
    "\n",
    "    for token in doc:\n",
    "        temp_sentence.append(token.text)\n",
    "        if token.dep_ == 'cc' or token.dep_ == 'punct':\n",
    "            simple_sentences.append(' '.join(temp_sentence).strip())\n",
    "            temp_sentence = []\n",
    "    \n",
    "    if temp_sentence:\n",
    "        simple_sentences.append(' '.join(temp_sentence).strip())\n",
    "    \n",
    "    # Further split based on commas and semicolons if needed\n",
    "    final_sentences = []\n",
    "    for sent in simple_sentences:\n",
    "        sub_sentences = nltk.sent_tokenize(sent.replace(',', '.').replace(';', '.'))\n",
    "        final_sentences.extend(sub_sentences)\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "# Example usage\n",
    "compound_sentence = \"It is raining like cats and dogs\"\n",
    "simple_sentences = compound_to_simple(compound_sentence)\n",
    "\n",
    "for idx, sent in enumerate(simple_sentences):\n",
    "    print(f\"Simple Sentence {idx+1}: {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Sentence 1: It is raining like dogs .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_by_conjunctions(sentence):\n",
    "    \"\"\"\n",
    "    Split a sentence by its conjunctions and return the part before the conjunction\n",
    "    and the phrases/words after the conjunction.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    before_conjunction = []\n",
    "    after_conjunctions = []\n",
    "    temp_sentence = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'cc':  # Coordinating conjunction\n",
    "            before_conjunction.append(' '.join(temp_sentence).strip())\n",
    "            temp_sentence = []\n",
    "        else:\n",
    "            temp_sentence.append(token.text)\n",
    "    \n",
    "    if temp_sentence:\n",
    "        after_conjunctions.append(' '.join(temp_sentence).strip())\n",
    "    \n",
    "    return before_conjunction, after_conjunctions\n",
    "\n",
    "def expand_phrases(before_conjunction, after_conjunctions):\n",
    "    \"\"\"\n",
    "    Expand phrases to form new simple sentences based on the parts before and after conjunctions.\n",
    "    \"\"\"\n",
    "    expanded_sentences = []\n",
    "    for part in before_conjunction:\n",
    "        if 'like' in part:\n",
    "            parts = part.split('like')\n",
    "            if len(parts) > 1:\n",
    "                for after_part in after_conjunctions:\n",
    "                    subjects = after_part.split('and')\n",
    "                    for subject in subjects:\n",
    "                        expanded_sentences.append(parts[0].strip() + ' like ' + subject.strip())\n",
    "            else:\n",
    "                expanded_sentences.append(part)\n",
    "        else:\n",
    "            for after_part in after_conjunctions:\n",
    "                expanded_sentences.append(part + ' ' + after_part)\n",
    "    return expanded_sentences\n",
    "\n",
    "def compound_to_simple(sentence):\n",
    "    # Split the sentence by conjunctions\n",
    "    before_conjunction, after_conjunctions = split_by_conjunctions(sentence)\n",
    "    \n",
    "    # Expand phrases based on the parts before and after conjunctions\n",
    "    final_sentences = expand_phrases(before_conjunction, after_conjunctions)\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "# Example usage\n",
    "compound_sentence = \"It is raining like cats and dogs.\"\n",
    "simple_sentences = compound_to_simple(compound_sentence)\n",
    "\n",
    "for idx, sent in enumerate(simple_sentences):\n",
    "    print(f\"Simple Sentence {idx+1}: {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before conjunction(s): ['A paragraph is a series of sentences that are organized']\n",
      "After conjunction(s): ['coherent .']\n",
      "Simple Sentence 1: A paragraph is a series of sentences that are organized coherent .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_by_conjunctions(sentence):\n",
    "    \"\"\"\n",
    "    Split a sentence by its conjunctions and return the part before the conjunction\n",
    "    and the phrases/words after the conjunction.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    before_conjunction = []\n",
    "    after_conjunctions = []\n",
    "    temp_sentence = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'cc':  # Coordinating conjunction\n",
    "            before_conjunction.append(' '.join(temp_sentence).strip())\n",
    "            temp_sentence = []\n",
    "        else:\n",
    "            temp_sentence.append(token.text)\n",
    "    \n",
    "    if temp_sentence:\n",
    "        after_conjunctions.append(' '.join(temp_sentence).strip())\n",
    "    \n",
    "    return before_conjunction, after_conjunctions\n",
    "\n",
    "def expand_phrases(before_conjunction, after_conjunctions):\n",
    "    \"\"\"\n",
    "    Expand phrases to form new simple sentences based on the parts before and after conjunctions.\n",
    "    \"\"\"\n",
    "    expanded_sentences = []\n",
    "    for part in before_conjunction:\n",
    "        if 'like' in part:\n",
    "            parts = part.split('like')\n",
    "            if len(parts) > 1:\n",
    "                for after_part in after_conjunctions:\n",
    "                    subjects = after_part.split('and')\n",
    "                    for subject in subjects:\n",
    "                        expanded_sentences.append(parts[0].strip() + ' like ' + subject.strip())\n",
    "            else:\n",
    "                expanded_sentences.append(part)\n",
    "        else:\n",
    "            for after_part in after_conjunctions:\n",
    "                expanded_sentences.append(part + ' ' + after_part)\n",
    "    return expanded_sentences\n",
    "\n",
    "def compound_to_simple(sentence):\n",
    "    # Split the sentence by conjunctions\n",
    "    before_conjunction, after_conjunctions = split_by_conjunctions(sentence)\n",
    "    \n",
    "    # Print parts before and after conjunctions\n",
    "    print(\"Before conjunction(s):\", before_conjunction)\n",
    "    print(\"After conjunction(s):\", after_conjunctions)\n",
    "    \n",
    "    # Expand phrases based on the parts before and after conjunctions\n",
    "    final_sentences = expand_phrases(before_conjunction, after_conjunctions)\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "# Example usage\n",
    "compound_sentence = \"A paragraph is a series of sentences that are organized and coherent.\"\n",
    "simple_sentences = compound_to_simple(compound_sentence)\n",
    "\n",
    "for idx, sent in enumerate(simple_sentences):\n",
    "    print(f\"Simple Sentence {idx+1}: {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "cats.\n",
      "cats dogs\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def split_compound_sentence(sentence):\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Identify the conjunction and split the sentence\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"CCONJ\":\n",
    "            # Find the start and end of the first part\n",
    "            start = token.head.left_edge.i\n",
    "            end = token.head.i\n",
    "            \n",
    "            first_part = doc[start:end+1].text\n",
    "            \n",
    "            # Create two new sentences\n",
    "            first_sentence = first_part + \".\"\n",
    "            second_sentence = first_part.rsplit(maxsplit=1)[0] + \" \" + doc[end+2:].text\n",
    "            \n",
    "            return first_sentence, second_sentence\n",
    "\n",
    "# Example usage\n",
    "input_sentence = \"It is raining like cats and dogs\"\n",
    "output_sentences = split_compound_sentence(input_sentence)\n",
    "print(\"Output:\")\n",
    "print(output_sentences[0])\n",
    "print(output_sentences[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
